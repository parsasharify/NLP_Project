{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b07eb33c"
      },
      "source": [
        "<div>\n",
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=220 height=220 align=left class=\"saturate\">\n",
        "\n",
        "<br>\n",
        "<font face=\"Times New Roman\">\n",
        "<div dir=ltr align=center> \n",
        "<!-- <font color=0F5298 size=7> -->\n",
        "<font color=0F5298 size=6>\n",
        "    Introduction to Machine Learning <br> <br>\n",
        "<!-- <font color=2565AE size=5> -->\n",
        "<font size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023 <br> <br>\n",
        "<font color=606060 size=5>\n",
        "    Homework 4: Practical - Naive Bayes <br> <br>\n",
        "<font color=686880 size=4>\n",
        "    TAs: Alireza Farashah - Arman Malekzadeh - Ali Salesi\n",
        "    \n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5a79508"
      },
      "source": [
        "### Full Name : Parsa Sharifi\n",
        "### Student Number : 99101762\n",
        "### Colab Link: https://colab.research.google.com/drive/17X2bxrBqn4jdjZhBWhJlw6PeIpurVtFx?usp=sharing\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTZBzFXNv8pP"
      },
      "source": [
        "Note 1: In this assignment, we are trying to simulate the functionality of something called \"CountVectorizer\" which is used in natural language processing. You are advised to take a look at this link before beginning to answer the questions:\n",
        "\n",
        "https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/\n",
        "\n",
        "Note 2: One or two TA sessions will be held related to this assignment to make sure you get familiarized with this topic. Therefore, keep calm and write code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsp-qF9vLbUt"
      },
      "source": [
        "# Download the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQG4cjKvK4zZ",
        "outputId": "d12891cf-bd34-44ae-a58b-d3f0f9e403cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 13:39:33--  https://www.dropbox.com/s/yf195wl0sp3term/dataset-train.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/yf195wl0sp3term/dataset-train.zip [following]\n",
            "--2023-05-08 13:39:34--  https://www.dropbox.com/s/dl/yf195wl0sp3term/dataset-train.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc25753e29240abd01378e395c28.dl.dropboxusercontent.com/cd/0/get/B7p-IyvwxPz4WL5XTToeUThjMFA4P0uvGs9yL7I04_k0zd8T6AZrcV38tqYcMtGU0EqegdT73YC05q90i9BgprK0XiEpXNJ1Ef8ED9Wlsk7-H4nwTo2lxKJQ9G8KVw08-91s4rUg7fAk43HSkrpow4Pyuzoh5WBJJ_54ROuN0v-_0aeRYFo7H5EHFdY5Igq6OHA/file?dl=1# [following]\n",
            "--2023-05-08 13:39:34--  https://uc25753e29240abd01378e395c28.dl.dropboxusercontent.com/cd/0/get/B7p-IyvwxPz4WL5XTToeUThjMFA4P0uvGs9yL7I04_k0zd8T6AZrcV38tqYcMtGU0EqegdT73YC05q90i9BgprK0XiEpXNJ1Ef8ED9Wlsk7-H4nwTo2lxKJQ9G8KVw08-91s4rUg7fAk43HSkrpow4Pyuzoh5WBJJ_54ROuN0v-_0aeRYFo7H5EHFdY5Igq6OHA/file?dl=1\n",
            "Resolving uc25753e29240abd01378e395c28.dl.dropboxusercontent.com (uc25753e29240abd01378e395c28.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc25753e29240abd01378e395c28.dl.dropboxusercontent.com (uc25753e29240abd01378e395c28.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170224802 (162M) [application/binary]\n",
            "Saving to: ‘/content/dataset.zip’\n",
            "\n",
            "/content/dataset.zi  74%[=============>      ] 121.69M  26.1MB/s    eta 2s     ^C\n"
          ]
        }
      ],
      "source": [
        "!wget -O /content/dataset.zip \"https://www.dropbox.com/s/yf195wl0sp3term/dataset-train.zip?dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkjLDi1xBR-r",
        "outputId": "28d50621-699b-4151-fb3e-c6b4ba662202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/dataset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/dataset.zip or\n",
            "        /content/dataset.zip.zip, and cannot find /content/dataset.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/dataset/\n",
        "!unzip '/content/dataset.zip' -d /content/dataset/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI_tN2UDLgT_"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xzZeY9pcBo-7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bovAIGr4LkcU"
      },
      "source": [
        "Read the csv file and load it as a dataframe."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I saved it in my google drive, if you want to run it again update the address"
      ],
      "metadata": {
        "id": "fHyeCie_nyQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o4HBXrd7n1UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XkCekbTQzaf",
        "outputId": "e051b22b-f679-41fc-fa2c-29bb13155394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       topic                                               text\n",
            "0     sports  به گزارش ورزش سه تیم فوتبال پدیده در روزی که ا...\n",
            "1   politics  پاسخ مثبت به انتقادات شورای نگهبان البته # قبل...\n",
            "2   politics  # دهم در پاسخ به سوالی درباره اینکه آیا سفر وی...\n",
            "3  economics  کنفرانس # بودن بازار خودرو و تاثیر آن بر اقتصا...\n",
            "4  economics  به گزارش ایسنا صحبت با برخی # # آن است که امرو...\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/dataset/content/news-train.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2eq6fRZZroS"
      },
      "source": [
        "# Dealing with Class Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__mdmjwgNSRb"
      },
      "source": [
        "For each \"topic\", count and display the number of news belonging to it. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f0oIh-PNYLB",
        "outputId": "ccd61477-9cec-4179-f83e-02d3add1d407"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sports       123105\n",
              "economics     49600\n",
              "cultural      13359\n",
              "politics      11297\n",
              "Name: topic, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "topic_counts = df['topic'].value_counts()\n",
        "topic_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ETq4g97Oj0c"
      },
      "source": [
        "Balance the dataset in a way that all of the topics get associated with the same number of news. For this purpose, find the topic for which the minimum number of news exists. Then, use downsampling to lower the number of news associated with the other topics.<br>\n",
        "Finally, save the result as a new dataframe. (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample"
      ],
      "metadata": {
        "id": "L5_AJzAP7sKZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XJawN5XRDFYf"
      },
      "outputs": [],
      "source": [
        "min_topic_count = topic_counts.min()\n",
        "\n",
        "df_balanced = pd.DataFrame()\n",
        "for topic in df['topic'].unique():\n",
        "    df_topic = df[df['topic'] == topic]\n",
        "    df_topic_downsampled = resample(df_topic, replace=False, n_samples=min_topic_count, random_state=42)\n",
        "    df_balanced = pd.concat([df_balanced, df_topic_downsampled])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4ZrwDjVg73y",
        "outputId": "9c0aa7ae-a854-48e2-cc34-845ffd06e1d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sports       11297\n",
              "politics     11297\n",
              "economics    11297\n",
              "cultural     11297\n",
              "Name: topic, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df_balanced['topic'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DMwlMiwZ4Ql"
      },
      "source": [
        "# Preparation and Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyrxWwjXQcfV"
      },
      "source": [
        "Split the dataframe to two parts for training (80%) and testing (20%). (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BXM2X_tJZIJ2"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df_balanced, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSzLXYMhaL3R"
      },
      "source": [
        "In this part, we will extract features from the textual data using a very basic method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCejmR4nhABv"
      },
      "source": [
        "Consider the following sentence:\n",
        "```\n",
        "Sometimes a dog can run faster than a cat.\n",
        "```\n",
        "In the above sentence, the \"tokens\" are:\n",
        "\n",
        "```\n",
        "\"sometimes\", \"a\", \"dog\", \"can\", \"run\", \"faster\", \"than\", \"a\", \"cat\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTnb43swgo8r"
      },
      "source": [
        "Find the tokens of each news article in the \"training dataframe\" by splitting it based on the occurence of the space character (5 points). For instance, the following sentence:\n",
        "```\n",
        "او به میدان رفت\n",
        "```\n",
        "gets converted to these tokens:\n",
        "```\n",
        "[\"او\",\"به\",\"میدان\",\"رفت\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dlCAqisb6cT",
        "outputId": "987128f9-3743-4f0c-d970-c01b72e979bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131394    [همه‌چیز, شبیه, مسابقه, هفته, بود, البته, بدون...\n",
            "88791     [رییس, مجمع, تشخیص, مصلحت, نظام, با, اشاره, به...\n",
            "121984    [مرحله, اول, مسابقات, لیگ, ورزش‌های, سه, گانه,...\n",
            "12673     [#, بارها, تاکید, کرده‌اند, #, برنامه‌ای, برای...\n",
            "106051    [به, گزارش, ورزش, سه, کمتر, کسی, فکر, می‌کرد, ...\n",
            "                                ...                        \n",
            "92968     [به, گزارش, ایسنا, جام‌جهانی, #, روسیه, #, روز...\n",
            "169808    [نمایش, کمدی, #, به, کارگردانی, میثم, عبدی, از...\n",
            "157638    [معاون, پژوهشی, جهاد, دانشگاهی, واحد, استان, ا...\n",
            "118852    [به, گزارش, ورزش, سه, آبی, پوشان, پایتخت, که, ...\n",
            "53822     [حسن, روحانی, روز, گذشته, در, بخشی, از, سخنان,...\n",
            "Name: text, Length: 36150, dtype: object\n"
          ]
        }
      ],
      "source": [
        "tokens = train_df['text'].apply(lambda x: x.split())\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD-UebxhiUyd"
      },
      "source": [
        "Count the tokens and list the top 5 frequent ones (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "QvplTHGS8J44"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NT374DNvbz9K"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['text'])\n",
        "tokenized_text = vectorizer.transform(train_df['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = tokenized_text.sum(axis = 0)\n",
        "number_of_tokens = [(token, total[0,index]) for token, index in vectorizer.vocabulary_.items()]\n",
        "number_of_tokens = sorted(number_of_tokens, key = lambda x: x[1], reverse = True)"
      ],
      "metadata": {
        "id": "zH8ahoD48l_4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_tokens[:5]"
      ],
      "metadata": {
        "id": "F9TSfPg080js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bc21ee-e523-4723-dd6d-c3b2342b7db7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('در', 743942),\n",
              " ('به', 609106),\n",
              " ('از', 435477),\n",
              " ('که', 425635),\n",
              " ('این', 424750)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJyE1s6Wi0ZH"
      },
      "source": [
        "Consider the following sentence:\n",
        "```\n",
        "تیم فوتبال بارسلونا امشب با بایرن مونیخ دیدار می‌کند.\n",
        "```\n",
        "In the above sentence, we do not need the word `با` to determine that the topic is \"sports\". This word doesn't have any impact on the category this sentence belongs to. \n",
        "This word and similar ones are called \"stopwords\". A list of stopwords for the Persian language can be found in this text file:\n",
        "https://github.com/ziaa/Persian-stopwords-collection/raw/master/Stopwords/Savoy/persianST.txt\n",
        "\n",
        "Use it to remove all tokens that are actually stopwords.\n",
        "Also, remove the `#` token. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xV_zvIvjtH1"
      },
      "outputs": [],
      "source": [
        "!wget -O /content/stopwords.txt https://github.com/ziaa/Persian-stopwords-collection/raw/master/Stopwords/Savoy/persianST.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I saved it in my google drive, if you want to run it again update the address"
      ],
      "metadata": {
        "id": "_sOySvaqnjW1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yni39LVMg1Hm",
        "outputId": "02011c6a-1120-4e22-a19c-512f925b41aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['اثر', 'البت', 'بالای', 'برابر', 'برای', 'بیرون', 'تول', 'توی', 'تی', 'جلوی', 'حدود', 'خارج', 'دنبال', 'روی', 'زیر', 'سری', 'سمت', 'سوی', 'ضد', 'طبق', 'عقب', 'عل', 'عنوان', 'قصد', 'لطفا', 'مد', 'نزد', 'نزدیک', 'وسط', 'پاعین', 'کنار'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/stopwords.txt', 'r', encoding='utf-8') as f:  \n",
        "    stop_words = f.read().splitlines()\n",
        "vectorizer = CountVectorizer(stop_words = stop_words)\n",
        "vectorizer = vectorizer.fit(train_df['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH6Lw66Xk-Vb"
      },
      "source": [
        "Make a list of the remaining unique tokens (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4OpoO5mw0mL",
        "outputId": "7a7d18aa-4000-4af5-99c3-360656b49c1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['آب', 'آبادان', 'آبان', ..., 'یکم', 'یکپارچه', 'یکی'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI9kbGYql-WU"
      },
      "source": [
        "Count the number of the occurences of each token in each document. This way, you can make a numpy array to represent each document. Call this new array `x_train`. (10 points)\n",
        "\n",
        "Example:\n",
        "\n",
        "Unique tokens in all documents: \n",
        "```\n",
        "0: hen\n",
        "1: sometimes\n",
        "2: a\n",
        "3: the\n",
        "4: dog\n",
        "5: runs\n",
        "6: faster\n",
        "7: than\n",
        "8: cat\n",
        "9: bird\n",
        "```\n",
        "\n",
        "Current Document: `sometimes a dog runs faster than a cat`\n",
        "\n",
        "The representation of the document:\n",
        "\n",
        "```\n",
        "[0, 1, 2, 0, 1, 1, 1, 1, 1, 0]\n",
        "```\n",
        "\n",
        "The meaning of this representation:\n",
        "\n",
        "```\n",
        "0: the document doesn't contain \"hen\"\n",
        "1: the document contains 1 \"sometimes\"\n",
        "2: the document contains 2 \"a\"\n",
        "0: the document doesn't contain \"the\"\n",
        "1: the document contains 1 \"dog\"\n",
        "1: the document contains 1 \"runs\"\n",
        "1: the document contains 1 \"faster\"\n",
        "1: the document contains 1 \"than\"\n",
        "1: the document contains 1 \"cat\"\n",
        "0: the document doesn't contain \"bird\"\n",
        "``` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "t2QMUJlHxJOc"
      },
      "outputs": [],
      "source": [
        "train_x = vectorizer.transform(train_df['text']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6e08A_7yKny",
        "outputId": "7c39e777-d74e-405b-bff9-260264f98d8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, ..., 0, 1, 3],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "train_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXeW76p2pjnc"
      },
      "source": [
        "Make another numpy array by converting the topics associated with the training news to numbers. For instance, if the topics are: `sports`, `economics`, `politics`, and `cultural`, convert them to `0` to `3`. Call this new array `y_train`. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wV5zeofEF3Nn"
      },
      "outputs": [],
      "source": [
        "train_y = train_df.topic.astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW72RP4G4nYG",
        "outputId": "4beac086-630a-4979-fd6e-34cf765711f2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131394    2\n",
              "88791     2\n",
              "121984    3\n",
              "12673     2\n",
              "106051    3\n",
              "         ..\n",
              "92968     3\n",
              "169808    0\n",
              "157638    0\n",
              "118852    3\n",
              "53822     2\n",
              "Length: 36150, dtype: int8"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lew_jlNPqhPb"
      },
      "source": [
        "Using the unique tokens you found in the training news, represent the testing news as another numpy array called `x_test`. (5 points)\n",
        "\n",
        "Note: Do not forget to remove stopwords and the `#` token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RIy2qMABrQcy"
      },
      "outputs": [],
      "source": [
        "test_x =  vectorizer.transform(test_df['text']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2MEkCKx5K3o",
        "outputId": "c1c7111c-4236-4984-d2ec-14b5ce474199"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 6],\n",
              "       [0, 0, 0, ..., 0, 0, 1],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWIRhxS_rX7M"
      },
      "source": [
        "Convert the topics associated with the testing news to their equivalent numbers (as before), and save the result as a numpy array called `y_test`. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "T0tXlWRHrWUI"
      },
      "outputs": [],
      "source": [
        "test_y = test_df.topic.astype('category').cat.codes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk2QwG6u5dgR",
        "outputId": "2949503b-47b2-4106-bd0b-1c0629a73374"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "177683    2\n",
              "158663    1\n",
              "99757     0\n",
              "39112     2\n",
              "17310     2\n",
              "         ..\n",
              "150793    3\n",
              "27165     1\n",
              "161829    0\n",
              "179421    3\n",
              "34480     1\n",
              "Length: 9038, dtype: int8"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQdf4UuQrvme"
      },
      "source": [
        "# Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwkWJbTEsoWy"
      },
      "source": [
        "## Using sklearn (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGcrjKBgr1uL"
      },
      "source": [
        "Train a Multinomial Naive Bayes Model on the training news (`x_train` and `y_train`) using `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yJTD0NB7rW2P"
      },
      "outputs": [],
      "source": [
        "model = MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_x, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UXJ_WsqW5osx",
        "outputId": "202e69e7-bac7-42b7-8eba-c6a4222cf549"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeFDzw61sCna"
      },
      "source": [
        "Get the predictions of the model for the testing news (`x_test`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "f0KP-84CsCJX"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1KAXbhJsb5o"
      },
      "source": [
        "Print the classification report containing \"precision\", \"recall\", and \"f1-score\" for each class, and their averages. (you can use `sklearn` for this part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1fLYOmgMsWns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f645bb9f-af8a-498e-8721-2914b6a5410e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.95      2255\n",
            "           1       0.91      0.94      0.93      2268\n",
            "           2       0.94      0.90      0.92      2262\n",
            "           3       0.99      0.98      0.99      2253\n",
            "\n",
            "    accuracy                           0.95      9038\n",
            "   macro avg       0.95      0.95      0.95      9038\n",
            "weighted avg       0.95      0.95      0.95      9038\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_y, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM7sjbu6stRn"
      },
      "source": [
        "## Using your own code (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Naive_Bayes:\n",
        "    \"\"\"\n",
        "    Fits it on data, then uses predict to get results.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the classifier.\n",
        "        \"\"\"\n",
        "        self.prior = None\n",
        "        self.conditional = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the training data\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : array-like, shape = [n_samples, n_features]\n",
        "            Training samples\n",
        "        y : array-like, shape = [n_samples, n_target_values]\n",
        "            Target values\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.priors = np.zeros(len(self.classes_))\n",
        "        for i, c in enumerate(self.classes_):\n",
        "            self.priors[i] = np.mean(y == c)\n",
        "\n",
        "        self.counts = np.zeros((len(self.classes_), n_features), dtype=int)\n",
        "        self.conditional = np.zeros((len(self.classes_), n_features))\n",
        "        for i, c in enumerate(self.classes_):\n",
        "            X_c = X[y == c]\n",
        "            for j in range(n_features):\n",
        "                self.counts[i, j] = np.sum(X_c[:, j])\n",
        "                # Apply smoothing by adding one to the count and dividing by the total count plus the number of features\n",
        "                self.conditional[i, j] = (self.counts[i, j] + 1) / (np.sum(self.counts[i]) + n_features)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "      if not hasattr(self, \"conditional\"):\n",
        "          raise ValueError(\"The model is not fitted yet. Call fit first.\")\n",
        "\n",
        "      n_samples, n_features = X.shape\n",
        "      if n_features != self.conditional.shape[1]:\n",
        "          raise ValueError(f\"Expected {self.conditional.shape[1]} features, got {n_features}\")\n",
        "\n",
        "      log_probs = np.zeros((n_samples, len(self.classes_)))\n",
        "      for i, c in enumerate(self.classes_):\n",
        "          log_probs[:, i] = np.log(self.priors[i]) + np.sum(np.log(self.conditional[i]) * X, axis=1)\n",
        "\n",
        "      return self.classes_[np.argmax(log_probs, axis=1)]\n"
      ],
      "metadata": {
        "id": "3PGkEs6rqKs4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYpwLACxtlFy"
      },
      "source": [
        "Train your own Multinomial Naive Bayes Model on the training news (`x_train` and `y_train`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QBnzQoqyIl-q"
      },
      "outputs": [],
      "source": [
        "my_model = Naive_Bayes()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.fit(np.array(train_x),np.array(train_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmgH5uwsGJry",
        "outputId": "931987ec-ca01-4942-efc2-f860883339b0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Naive_Bayes at 0x7fa62800e1a0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPSZVHBCtndS"
      },
      "source": [
        "Get the predictions of your model for the testing news (`x_test`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "T7SMtuR0tskf"
      },
      "outputs": [],
      "source": [
        "my_model_pred = my_model.predict(np.array(test_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIlERTP1ttCW"
      },
      "source": [
        "Print the classification report containing \"precision\", \"recall\", and \"f1-score\" for each class, and their averages. (you can use `sklearn` for this part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9MfxJtbmtugx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f5825f-2508-47e4-e83a-90a8454a4f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.96      0.94      2255\n",
            "           1       0.91      0.95      0.93      2268\n",
            "           2       0.95      0.90      0.92      2262\n",
            "           3       0.99      0.99      0.99      2253\n",
            "\n",
            "    accuracy                           0.95      9038\n",
            "   macro avg       0.95      0.95      0.95      9038\n",
            "weighted avg       0.95      0.95      0.95      9038\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(test_y, my_model_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElMEcOMAeFA1"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64J1wFXweGsH"
      },
      "source": [
        "Competition Link: https://www.kaggle.com/t/88f1b6e251e34575b2e4cb4b91aed0ef"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}